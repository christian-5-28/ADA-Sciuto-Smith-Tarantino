{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-3cbc4dca2afe>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mwordcloud\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSTOPWORDS\u001b[0m \u001b[1;31m# install this - pip name is just 'wordcloud'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import folium\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from wordcloud import WordCloud, STOPWORDS # install this - pip name is just 'wordcloud'\n",
    "from PIL import Image\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "#plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Exploration of Trump's Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total tweets per month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let us read the condensed json files with tweets from 2009 to present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = [\"trump_tweets/condensed_20{:0>2}.json\".format(yr) for yr in range(9, 18)]\n",
    "\n",
    "df_0917 = pd.DataFrame()\n",
    "\n",
    "for file in filenames:\n",
    "    df_0917 = pd.concat([df_0917, pd.read_json(file)])\n",
    "\n",
    "stamps = df_0917.created_at\n",
    "len(df_0917)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regroup by monthly total and plot.  \n",
    "Resample keys: http://pandas.pydata.org/pandas-docs/stable/timeseries.html#offset-aliases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_idx = pd.DatetimeIndex(stamps)\n",
    "times = pd.Series([1]*len(date_idx), index=date_idx)\n",
    "tweet_times = pd.DataFrame({'tweet_count':times})\n",
    "\n",
    "monthly = tweet_times.resample('M').sum()\n",
    "fig2, ax2 = plt.subplots(1,1, figsize=(16,6))\n",
    "ax2.bar(monthly.index.values, monthly[\"tweet_count\"].values, width=10, color=\"#0f83ff\")\n",
    "ax2.plot(monthly[\"tweet_count\"], color=\"#5bc3ff\")\n",
    "ax2.set_title(\"Total tweets per month from 2009 to present\")\n",
    "ax2.set_xlabel(\"Month\")\n",
    "ax2.set_ylabel(\"Number of tweets\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting too see that total tweets per month were *not* at their highest level during his campaign (June 16, 2015 to November 9, 2016). Maybe only the content changed? We will explore this further down below.\n",
    "\n",
    "Also, note the sharp decline right after the election inNovember 2016."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distrubution by hour of day\n",
    "\n",
    "Before we begin analysing word usage, let us first take a look at in what hour of the day Trump's tweets are posted using the entire dataset from 2009 to present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "times = tweet_times\n",
    "df1 = times.pivot_table(index=times.index.hour,\n",
    "                        values='tweet_count', \n",
    "                        aggfunc='sum')\n",
    "\n",
    "df1 = df1/df1.max() # Normalize to 1. Comment: change normfactor to total tweets?\n",
    "\n",
    "fig3, ax3 = plt.subplots(1,1, figsize=(10,8))\n",
    "ax3.bar(df1.index.values, df1[\"tweet_count\"], width=0.1, color=\"#0f83ff\")\n",
    "ax3.plot(df1[\"tweet_count\"], color=\"#5bc3ff\")\n",
    "ax3.set_title(\"Tweets per hour of day from 2016 to present \")\n",
    "ax3.set_xlabel(\"Time of day\")\n",
    "ax3.set_ylabel(\"Tweets (normalized by max)\")\n",
    "plt.xticks(np.arange(24))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this is not very interesting on its own.  \n",
    "*Discuss timezones?*\n",
    "\n",
    "Let us plot this distribution for three different periods instead.\n",
    "\n",
    " * Before campaign\n",
    " * During campaign\n",
    " * After campaign (presidency)\n",
    "\n",
    "#### Distribution for three periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times_before = tweet_times.loc[(tweet_times.index < \"2015-06-16 00:00:01\")]\n",
    "times_election = tweet_times.loc[(tweet_times.index > \"2015-06-16 00:00:01\") \n",
    "                                 & (tweet_times.index < \"2016-11-09 23:59:59\")]\n",
    "times_pres = tweet_times.loc[(tweet_times.index > \"2016-11-09 00:00:01\")]\n",
    "\n",
    "def times_to_hour(tweet_times_series):\n",
    "    df_temp = tweet_times_series.pivot_table(\n",
    "                    index=tweet_times_series.index.hour,\n",
    "                    values='tweet_count', \n",
    "                    aggfunc='sum')\n",
    "    return df_temp/df_temp.max()\n",
    "\n",
    "df_b = times_to_hour(times_before)\n",
    "df_e = times_to_hour(times_election)\n",
    "df_p = times_to_hour(times_pres)\n",
    "\n",
    "fig3, ax3 = plt.subplots(1,1, figsize=(10,8))\n",
    "ax3.plot(df_b[\"tweet_count\"], 'r.-')\n",
    "ax3.plot(df_e[\"tweet_count\"], 'b.-')\n",
    "ax3.plot(df_p[\"tweet_count\"], 'g.-')\n",
    "plt.legend([\"Before campaign (< 2015-06-16) \", \"During campaign\", \"After campaign (> 2016-11-09)\"])\n",
    "ax3.set_title(\"Tweets per hour of day for different time periods\")\n",
    "ax3.set_xlabel(\"Time of day\")\n",
    "ax3.set_ylabel(\"Tweets (normalized by max)\")\n",
    "plt.xticks(np.arange(24))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that during the campaign, compared to before and after, we have a much more constant flow of tweets throughout the whole day. The tweets during the campaign also continue into the night, while before and after the number of tweets decrease as the night approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distrubution by device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following devices are listed in the \"source\" column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(df_0917.source.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the vast majority are either from Android, Web, or iPhone.\n",
    "\n",
    "Will treat TweetDeck and TwitLonger as Twitter Web Client. Then split by Android, iPhone, and Web.\n",
    "Ignore everything else since it is such a small part of all tweets.\n",
    "\n",
    "*Note*  \n",
    "Might be interesting to look at these unusual sources by themselves, not something we have planned on doing though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "stamps_android = df_0917.loc[df_0917[\"source\"] == \"Twitter for Android\"].created_at\n",
    "stamps_iphone = df_0917.loc[df_0917[\"source\"] == \"Twitter for iPhone\"].created_at\n",
    "stamps_web = df_0917.loc[(df_0917[\"source\"] == \"Twitter Web Client\") \n",
    "            | (df_0917[\"source\"] == \"TweetDeck\") \n",
    "            | (df_0917[\"source\"] == \"TwitLonger Beta\")].created_at\n",
    "\n",
    "times_android = pd.DataFrame({\"tweet_count\" : [1]*len(stamps_android)}, index=pd.DatetimeIndex(stamps_android))\n",
    "times_iphone = pd.DataFrame({\"tweet_count\" : [1]*len(stamps_iphone)}, index=pd.DatetimeIndex(stamps_iphone))\n",
    "times_web = pd.DataFrame({\"tweet_count\" : [1]*len(stamps_web)}, index=pd.DatetimeIndex(stamps_web))\n",
    "\n",
    "df_android = times_to_hour(times_android)\n",
    "df_iphone = times_to_hour(times_iphone)\n",
    "df_web = times_to_hour(times_web)\n",
    "\n",
    "# Plot results\n",
    "fig4, ax4 = plt.subplots(1,1, figsize=(10,8))\n",
    "ax4.plot(df_android[\"tweet_count\"], '.-', color=\"#44ce42\")\n",
    "ax4.plot(df_iphone[\"tweet_count\"], '.-', color=\"#595959\")\n",
    "ax4.plot(df_web[\"tweet_count\"], '.-', color=\"#81b8e8\")\n",
    "plt.legend([\"Android\", \"iPhone\", \"Web\"])\n",
    "ax4.set_title(\"Tweets per hour of day for different device sources\")\n",
    "ax4.set_xlabel(\"Time of day\")\n",
    "ax4.set_ylabel(\"Tweets (normalized by max)\")\n",
    "plt.xticks(np.arange(24))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a clear difference between iPhone and Android. It does not seem like Trump use brand during work and another during time off. No, if thas was the case then the curves for iPhone and Android would indicate a clear around some time of day. It also does not seem like that Trump alternate in a consistent manner between iPhone and Android; the iPhone tweets continue in the evening as the Android ones are in sharp decline.  \n",
    "This discrepancy in device usage is interesting and will be explored further later.\n",
    "\n",
    "Also note that the web client is pretty much only used during office hours."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geolocation of Trump's tweet during campaign."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code goes here. Make map (cloropleth or markers) from location tagged tweets in campaign.\n",
    "# If this is not filled in, it will be completed before milestone 3 but was not completed before milestone 2.\n",
    "\n",
    "# Map might indicate most important states or states he prefers to visit (the ones where he has the most support?).\n",
    "loc_tagged_tweets = pd.read_csv(\"election_tweets/election_tweets.csv\")\n",
    "loc_tagged_tweets.loc[loc_tagged_tweets[\"handle\"] == \"realDonaldTrump\"][\"place_full_name\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO  \n",
    "Change masks and colours to something interesting. Also color words from image. E.g. use shape of Trump's head with \"correct\" colours for campaign word usage.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_wordcloud(tweet_series, replace_tuples=[], mask_path=None, word_limit=250, font_size_limit=70):\n",
    "    cloud_txt = \" \\n \".join(tweet_series.values)\n",
    "    for s_find, s_replace in replace_tuples:\n",
    "        cloud_txt = re.sub(s_find, s_replace, cloud_txt)\n",
    "        \n",
    "    stopwords = set(STOPWORDS)\n",
    "    if mask_path is not None:\n",
    "        cloud_mask = np.array(Image.open(mask_path))\n",
    "    \n",
    "    return WordCloud(background_color=\"white\", max_words=word_limit,\n",
    "                     stopwords=stopwords, mask=cloud_mask,\n",
    "                     max_font_size=font_size_limit).generate(cloud_txt)\n",
    "\n",
    "\n",
    "def plot_wordcloud(cloud, size_tuple=(16,8)):\n",
    "    fig, ax = plt.subplots(figsize=size_tuple)\n",
    "    ax.imshow(cloud, interpolation=\"bilinear\")\n",
    "    ax.axis(\"off\")\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_tweets = pd.read_csv(\"election_tweets/election_tweets.csv\")\n",
    "dt = all_tweets.loc[(all_tweets[\"handle\"] == \"realDonaldTrump\") & (all_tweets[\"is_retweet\"] == False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wordcloud of Trump tweets during campaign\n",
    "\n",
    "Note the TODO above.\n",
    "\n",
    "Phrases of interest are phrases such as \"Crooked Hillary\", \"Make America Great Again\", and a large proportion of \"will\" and \"people\". Note the appearance of \"poll\" during the election."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc = make_wordcloud(dt[\"text\"], mask_path=\"images/usa_stencil.jpg\",\n",
    "                    replace_tuples=[(r\"https:\\S*\", \"\"),(\"realDonaldTrump\", \"\"),(\"amp\", \"\")])\n",
    "plot_wordcloud(wc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wordcloud of Trump tweets after election\n",
    "\n",
    "The rise of the term \"Fake News\" is explored thoroughly later. Note the disappearance of Hillary related words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_1617 = pd.DataFrame()\n",
    "for file in [\"trump_tweets/condensed_2016.json\", \"trump_tweets/condensed_2017.json\"]:\n",
    "    dt_1617 = pd.concat([dt_1617, pd.read_json(file)])\n",
    "\n",
    "tweets_after = dt_1617.loc[(dt_1617[\"created_at\"] > \"2016-11-10 00:00:01\") & (dt_1617[\"is_retweet\"] == False)]\n",
    "\n",
    "after_text = tweets_after[\"text\"]\n",
    "after_text = \" \\n \".join(after_text.values)\n",
    "after_text = re.sub(r\"https:\\S*\", \"\", after_text)\n",
    "after_text = re.sub(\"amp\", \"\", after_text)\n",
    "\n",
    "usa_mask = np.array(Image.open(\"images/usa_stencil.jpg\"))\n",
    "stopwords = set(STOPWORDS)\n",
    "\n",
    "wc = WordCloud(background_color=\"white\", max_words=250,\n",
    "               stopwords=stopwords, mask=usa_mask,\n",
    "               max_font_size=70).generate(after_text)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "\n",
    "ax.imshow(wc, interpolation=\"bilinear\")\n",
    "ax.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wordcloud of Trump tweets before election\n",
    "\n",
    "Here we have mentions of Obama (Obama/president) instead of Hillary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "before_filenames = filenames = [\"trump_tweets/condensed_20{:0>2}.json\".format(yr) for yr in range(9, 16)]\n",
    "\n",
    "dt_0915 = pd.DataFrame()\n",
    "for file in before_filenames:\n",
    "    dt_0915 = pd.concat([dt_0915, pd.read_json(file)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tweets_before = dt_0915.loc[(dt_0915[\"created_at\"] < \"2016-04-11 23:59:59\") & (dt_0915[\"is_retweet\"] == False)]\n",
    "\n",
    "\n",
    "wc = make_wordcloud(tweets_before[\"text\"], mask_path=\"images/usa_stencil.jpg\",\n",
    "                    replace_tuples=[(r\"https:\\S*\", \"\"),\n",
    "                                    (\"realDonaldTrump\", \"\"),\n",
    "                                    (\"amp\", \"\"), (\"co\", \"\"), (\"BarackObama\", \"\")])\n",
    "plot_wordcloud(wc, size_tuple=(16,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Detection with LDA (Latent Dirichlet allocation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from helpers import *\n",
    "import operator\n",
    "import collections\n",
    "import glob\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data\n",
    "* Here we load the stopwords (their usage will be explained later) and all the data through an helper function load_data(). This function load all the data in one dictionary (all data) and two lists (condensed and master) and returns them. The dictionary values and the lists elements are pandas DataFrames. For what concerns the dictionary, we can access a DataFrame that refers to a json file using the file name without the .json extension. E.g.: all_data[\"condensed_2009\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('stopwords/stopwords_joined.json', 'r') as f:\n",
    "         stopwords = json.load(f)\n",
    "        \n",
    "all_data, condensed, master = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We decided to use only the condensed tweets and not the master ones because in the latter ones there are a lot of informations that are not useful to our analysis. Every DataFrame in the condensed list contains all the tweets of one specific year (from 2009 to 2017).\n",
    "* As explained in David Robertson's [article](http://varianceexplained.org/r/trump-tweets/)Trump tweets from an Android phone or from a laptop. All the tweets from an iPhone are tweets made by his staff, so we decided to exclude them from our analysis. We excluded as well the retweets, because they are not preparatory for studying Trump's behavior.\n",
    "* Note: after a [further analysis](#phone_analysis), we found out that Trump changed his mobile phone to an iPhone on March 2017. Every result reported in this section is still valid, but until March 2017.\n",
    "* So what we are going to do is to save every text from the tweets in a list and all the info about those tweets in another list. In this way we can work with text only elements and when we will need them, it will be easy to link the text to their additional info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_flat_list(list_):\n",
    "    '''Helper function that transforms a list of list in a flat list'''\n",
    "    return [item for sublist in list_ for item in sublist]\n",
    "\n",
    "# This list will contain the texts from every tweet chosen\n",
    "condensed_text = []\n",
    "# This list will contain the dates from every tweet chosen\n",
    "condensed_date = []\n",
    "# This list will contain the retweet counts from every tweet chosen\n",
    "condensed_retweet_count = []\n",
    "# This list will contain the favorite counts from every tweet chosen\n",
    "condensed_favorite_count = []\n",
    "# This list will contain the ids from every tweet chosen\n",
    "condensed_id = []\n",
    "\n",
    "'''\n",
    "Now, for every element (DataFrame) in the condensed list, we take the informations \n",
    "above explained and we put them in the right list.\n",
    "We don't choose tweets that are retweet or that are from an iPhone.\n",
    "'''\n",
    "for x in condensed:\n",
    "    temp = x[x.is_retweet == False]\n",
    "    temp = temp[temp.source != \"Twitter for iPhone\"]\n",
    "    condensed_text.append(temp.text.tolist())\n",
    "    condensed_date.append(temp.created_at.tolist())\n",
    "    condensed_retweet_count.append(temp.retweet_count.tolist())\n",
    "    condensed_favorite_count.append(temp.favorite_count.tolist())\n",
    "    condensed_id.append(temp.id_str.tolist())\n",
    "\n",
    "'''\n",
    "Since we don't want to have a list of lists (every list inside the list now contains\n",
    "the specific data (text, date, ...) for an entire year: we don't want to make a difference\n",
    "between the years now), we make every list a flat list.\n",
    "'''\n",
    "flat_list_text = to_flat_list(condensed_text)\n",
    "flat_list_date = to_flat_list(condensed_date)\n",
    "flat_list_retweet_count = to_flat_list(condensed_retweet_count)\n",
    "flat_list_favorite_count = to_flat_list(condensed_favorite_count)\n",
    "flat_list_id = to_flat_list(condensed_id)\n",
    "\n",
    "'''\n",
    "Evert additional information (i.e. everything that is not the text of a tweet) is saved\n",
    "in a list of tuples. Every tuple is composed by (date, retweet_count, favorite_count, id).\n",
    "The elements on the same index of the two lists (flat_list_text and flat_list_info) refer\n",
    "to the same tweet.\n",
    "'''\n",
    "flat_list_info = []\n",
    "for date, ret, fav, id_ in zip(flat_list_date, flat_list_retweet_count, flat_list_favorite_count, flat_list_id):\n",
    "    flat_list_info.append((date, ret, fav, id_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* From now on, we will start the process of 'clustering' the texts in different topics.\n",
    "* Additional note: before using the stopwords, we tried to use stemming on the text. Stemming is the process that reduce every word to its root. This method didn't give us the expected results. If you are interested in the stemming process, we used PorterStemmer from nltk library. You can find all the documentation [here](http://www.nltk.org/api/nltk.stem.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We use a CountVectorized (from skelearn) that convert a collection of text documents to a matrix of token counts. We chose a max_features = 1000 in order to take account of only the 1000 most used words in the bag of words matrix.\n",
    "* The stopwords used in this script are an union between handmade stopwords and the stopwords that you can get [here](https://www.ranks.nl/stopwords) - in the section 'Long Stopword List'. Every word inside the stopwords is not used to cluster the tweets. They are really useful because without them we could find tweets in the same topic but that refer to different topics. For example, 'the' is a very common stopword, because it doesn't give any information about the topic of the tweet. If we take account of that word during the clustering, two tweets that have a lot of 'the' in their content may finish in the same topic, but it's not right!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "no_features = 1000\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=no_features, stop_words=stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now we are going to call two methods from the tf_vectorizer: fit_transform and get_features_names. The first one learn the vocabulary dictionary and return the term-document matrix. Every row of the matrix refer to a term and every column refer to a document. Every value is given by: tf_i,j * idf_i.\n",
    "* Let's explain them: tf_i,j is the term frequency of the i_th term on the j_th document, so tf_i,j = Nij/Dj, where Nij is the occurency of term i in document j and Dj is the length (number of words) of the document j; idf_i is the inverse document frequency and so idf_i = log10(D/number_of_documents_that_contain_i): D is the total number of documents and the denominator is self explaining. So the idf_i factor explains the importance of a term in that document. Here is an example: let's say that we have 10 documents and we take the 3rd one. In the 3rd one we choose the word 'cat', which is repeated 20 times, so tf_cat,3 = 20. We can find the word 'cat' in only 1 other document, so this means that cat is very important to describe this document, since it's very rare to find it in other documents. That's why the idf_cat will be very high: idf_cat = log10(20/2) = 1.\n",
    "* get_features_names() returns an array mapping from feature integer indices to feature name, or, in other words, the 1000 words that have been chosen by the CountVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf = tf_vectorizer.fit_transform(flat_list_text)\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Unluckily, the LDA algorithm can't find automatically the number of topics and so we need to give him the number of total topics that we have to find. Before finding the right number of topics and the right stopwords in order to have coherence in our topics, we had to try many times. The final result is that if we divide all the texts in 8 topics and then work a little bit with them, we find good coherence in the texts of the same topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "no_topics = 8\n",
    "lda_model = LatentDirichletAllocation(n_components=no_topics, max_iter=5, learning_method='online', learning_offset=50.,random_state=0).fit(tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* At this point, in order to show our results, we need to find the W (topics to documents) and H (word to topics) matrices. H: every row is a topic, every column is a word (columns length = no_features); the elements inside the matrix contain the weight of every word about the topic. W: every row is a document, every column is a topic; the elements inside the matrix contain the weight of every topic in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda_W = lda_model.transform(tf)\n",
    "lda_H = lda_model.components_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Here we define a function that shows the first three words and three documents and their scores of every topic. If you want to see more documents, just change the variables number_of_words and number_of_documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def show_topics(H, W, feature_names, documents, number_of_words, number_of_documents):\n",
    "    '''\n",
    "    If we do an inverse argsort of a topic (of a row) of matrix H, that returns the indices of the ordered list\n",
    "    of the scores/weights and we use those indices to extract words from feature_names.\n",
    "    These words will be ordered from the most to the least important.\n",
    "    We did the same reasoning for the W matrix (documents)\n",
    "    '''\n",
    "    \n",
    "    for topic_idx, topic in enumerate(H):\n",
    "        print(\"Topic %d:\" % (topic_idx))\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "        top_doc_indices = np.argsort(W[:, topic_idx])[::-1][0:no_top_documents]\n",
    "        for doc_index in top_doc_indices:\n",
    "            print(documents[doc_index])\n",
    "            print('Score: ' + str(W[doc_index, topic_idx]))\n",
    "            print('\\n')\n",
    "            \n",
    "no_top_words = 3\n",
    "no_top_documents = 3\n",
    "show_topics(lda_H, lda_W, tf_feature_names, flat_list_text, no_top_words, no_top_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We looked through the documents in the topics and we found out that the 8 topics are divided as follows:\n",
    "     <p>0) POLITICS\n",
    "     <p>1) HOTELS AND GOLF\n",
    "     <p>2) POLITICS\n",
    "     <p>3) CELEBRITY APPRENTICE AND SHOWS\n",
    "     <p>4) BOOKS, YANKEES, QUOTATIONS\n",
    "     <p>5) GOLF, TRUMP'S BUSINESS\n",
    "     <p>6) QUATATIONS, COMMENTS OF DIFFERENT KIND\n",
    "     <p>7) INTERVIEWS AND DEBATES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can see that the clustering is not perfect (category 4 and 6 are a little confused), but we are having the first results. We can see that not always there is a strong correspondance between the most common words and the real meaning of the topic, but we will take care of that.\n",
    "* First we will put together topics 1-5 in a TRUMP'S BUSINESS CATEGORY: this new cluster will contain topics such as golf, hotels, Trump's cologne, Trump Signature Collection... Secondly we will put together topics 4-6 in a VARIOUS category: this new cluster will contain topics such as general tweets, books, quotations... Lastly, we will put together topics 0-2: this new topic will contain everything that refers to politics.\n",
    "* While putting everything together, we will have to be careful in choosing the documents that refer to that topic with a score at least of the 60% in order to mantain coherence. For this reason we will not consider documents for a topic under that threshold.\n",
    "* NOTE: we can have the same document in more topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "business = []\n",
    "scores_business = []\n",
    "various = []\n",
    "scores_various = []\n",
    "politics = []\n",
    "scores_politics = []\n",
    "shows = []\n",
    "scores_shows = []\n",
    "interviews = []\n",
    "scores_interviews = []\n",
    "\n",
    "number_of_tweets_kept = 0\n",
    "'''In every topic list we append the text and the index of the tweet in order to get the info later'''\n",
    "for topic_idx, topic in enumerate(lda_H):\n",
    "\n",
    "    # Taking the indices of the documents ordered by importance in the topic\n",
    "    top_doc_indices_ordered = np.argsort(lda_W[:, topic_idx])[::-1]\n",
    "\n",
    "    for doc_index in top_doc_indices_ordered:\n",
    "        score = lda_W[doc_index, topic_idx]\n",
    "\n",
    "        # If the documents are less important than 0.60, I finish collecting documents for that topic\n",
    "        if (score < 0.60):\n",
    "            break\n",
    "\n",
    "        # BUSINESS\n",
    "        if topic_idx == 1 or topic_idx == 5:\n",
    "\n",
    "            business.append([flat_list_text[doc_index], doc_index])\n",
    "            scores_business.append(score)\n",
    "\n",
    "        # VARIOUS\n",
    "        elif topic_idx == 4 or topic_idx == 6:\n",
    "\n",
    "            various.append([flat_list_text[doc_index], doc_index])\n",
    "            scores_various.append(score)\n",
    "\n",
    "        # POLITICS\n",
    "        elif topic_idx == 0 or topic_idx == 2:\n",
    "            politics.append([flat_list_text[doc_index], doc_index])\n",
    "            scores_politics.append(score)\n",
    "\n",
    "        # CELEBRITY APPRENTICE AND SHOWS\n",
    "        elif topic_idx == 3:\n",
    "\n",
    "            shows.append([flat_list_text[doc_index], doc_index])\n",
    "            scores_shows.append(score)\n",
    "\n",
    "        # INTERVIEWS AND DEBATES\n",
    "        elif topic_idx == 7:\n",
    "\n",
    "            interviews.append([flat_list_text[doc_index], doc_index])\n",
    "            scores_interviews.append(score)\n",
    "        \n",
    "        number_of_tweets_kept += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now let's see what happened! We will show just the first 5 documents in the topic, but if you are curious to see more do not hesitate to see more documents just changing the max_document value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_document = 5\n",
    "'''PRINT BUSINESS TOPIC'''\n",
    "scores_business_idx_ordered = np.argsort(scores_business)[::-1][:max_document]\n",
    "print(\"NEW TOPIC: BUSINESS\")\n",
    "for index in scores_business_idx_ordered:\n",
    "    print(business[index])\n",
    "    print(scores_business[index])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''PRINT VARIOUS TOPIC'''\n",
    "scores_various_idx_ordered = np.argsort(scores_various)[::-1][:max_document]\n",
    "print(\"NEW TOPIC: VARIOUS \")\n",
    "for index in scores_various_idx_ordered:\n",
    "    print(various[index])\n",
    "    print(scores_various[index])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''PRINT POLITICS TOPIC'''\n",
    "scores_politcs_idx_ordered = np.argsort(scores_politics)[::-1][:max_document]\n",
    "print(\"NEW TOPIC: POLITICS \")\n",
    "for index in scores_politcs_idx_ordered:\n",
    "    print(politics[index])\n",
    "    print(scores_politics[index])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''PRINT SHOWS TOPIC'''\n",
    "scores_shows_idx_ordered = np.argsort(scores_shows)[::-1][:max_document]\n",
    "print(\"NEW TOPIC: SHOWS \")\n",
    "for index in scores_shows_idx_ordered:\n",
    "    print(shows[index])\n",
    "    print(scores_shows[index])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''PRINT INTERVIEWS AND DEBATES TOPIC'''\n",
    "scores_interviews_idx_ordered = np.argsort(scores_interviews)[::-1][:max_document]\n",
    "print(\"NEW TOPIC: INTERVIEWS AND DEBATES \")\n",
    "for index in scores_interviews_idx_ordered:\n",
    "    print(interviews[index])\n",
    "    print(scores_interviews[index])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Since we have chosen to keep only the documents that refered to one topic with a score of 60%, we are keeping only ~14% of the total tweets in the topic detection. We think that is fine, because the analysis that refer to the topics is not influenced by other tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(flat_list_text), number_of_tweets_kept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of tweets in topic politics: ', len(politics))\n",
    "print('Number of tweets in topic business: ', len(business))\n",
    "print('Number of tweets in topic various: ', len(various))\n",
    "print('Number of tweets in topic shows: ', len(shows))\n",
    "print('Number of tweets in topic interviews: ', len(interviews))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can clearly see that the topic with more tweets is the politic one. But if we make a research for the words 'hillary', 'clinton', 'crooked', we can find them only in 15 tweets! Probably the Clinton topic was not recognized in our clustering, but it's not a problem. We can make a keyword research from all the topics and find the ones that talk about Hillary Clinton using an helper function get_hillary_tweets_16_17. We are going to take the tweets of the last 2 years because that is the period of the elections and presidency of Trump, when he had to face his opponent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hillary_words = ['hillary', 'clinton', 'crooked']\n",
    "number_of_tweets_hillary = 0\n",
    "for tweet in politics:\n",
    "    for word in hillary_words:\n",
    "        if word in tweet[0].lower():\n",
    "            number_of_tweets_hillary += 1\n",
    "            break\n",
    "number_of_tweets_hillary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hillary_topic, hillary_info = get_hillary_tweets_16_17(all_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Since politics is a very big topic (it contains more than 1500 documents) we decided to split it in the most common Trump's topic about politics: China, Obama, foreign politics (contains also China topic), internal politics (contains also Obama topic) and Trump topic. The last one refers to the candidacy, the electoral campaign and tweets of political ideas of Trump. In order to do so we will load the keywords that refer to these topics (and also the keywords to find Hillary tweets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('keywords/china_keywords.json', 'r') as fp:\n",
    "    china_keywords = json.load(fp)\n",
    "    \n",
    "with open('keywords/obama_keywords.json', 'r') as fp:\n",
    "    obama_keywords = json.load(fp)\n",
    "    \n",
    "with open('keywords/hillary_keywords.json', 'r') as fp:\n",
    "    hillary_keywords = json.load(fp)\n",
    "    \n",
    "with open('keywords/foreign_politics_keywords.json', 'r') as fp:\n",
    "    foreign_politics_keywords = json.load(fp)\n",
    "    \n",
    "with open('keywords/internal_politics_keywords.json', 'r') as fp:\n",
    "    internal_politics_keywords = json.load(fp)\n",
    "    \n",
    "with open('keywords/trump_keywords.json', 'r') as fp:\n",
    "    trump_keywords = json.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In the next lines of code, we are going to detect if at least one of the keywords of the topic is inside the text of the tweet. If so we will add that tweet to his topic list and we will search for the keywords of another topic. If we found one keyword, we add the text and then break the for loop on the keywords because the topic is already been detected. If a tweet is not put in any topic, it will finish in the not_got list. The not_got list is a way to clean a little more our clusters.\n",
    "* Before doing everything explained above, we are going to remove the tweets that refer to Hillary Clinton from the politcs topic. We are doing this because every topic that refer to Hillary is already in the Hillary topic and we don't want duplicates\n",
    "* The commentated code is the code that we used to create the json file that we will use later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Removing the tweets in politics that contain hillary, because we are going to add every tweet that refers to hillary\n",
    "# later and we don't want to have duplicates\n",
    "for tweet in list(politics):\n",
    "    for hil_word in hillary_keywords:\n",
    "        if hil_word in tweet[0]:\n",
    "            politics.remove(tweet)\n",
    "            break\n",
    "\n",
    "\n",
    "china_topic = []\n",
    "obama_topic = []\n",
    "foreign_politics_topic = []\n",
    "internal_politics_topic = []\n",
    "trump_4pre_topic = []\n",
    "not_got = []\n",
    "\n",
    "for tweet in politics:\n",
    "\n",
    "    control = 0\n",
    "\n",
    "    # date, ret, fav, id_ = flat_list_info[tweet[1]]\n",
    "    # json_file = {\n",
    "    #     'created_at': str(date),\n",
    "    #     'id': id_,\n",
    "    #     'retweet_count': ret,\n",
    "    #     'favorite_count': fav,\n",
    "    #     'text': tweet[0],\n",
    "    #     'topic': ''\n",
    "    # }\n",
    "\n",
    "    for keyword in china_keywords:\n",
    "        if keyword in tweet[0]:\n",
    "            china_topic.append(tweet)\n",
    "\n",
    "            # json_file['topic'] = 'china'\n",
    "            # china_json.append(json_file.copy())\n",
    "\n",
    "            control += 1\n",
    "            break\n",
    "\n",
    "    for keyword in obama_keywords:\n",
    "        if keyword in tweet[0]:\n",
    "            obama_topic.append(tweet)\n",
    "\n",
    "            # json_file['topic'] = 'obama'\n",
    "            # obama_json.append(json_file.copy())\n",
    "\n",
    "            control += 1\n",
    "            break\n",
    "\n",
    "    for keyword in foreign_politics_keywords:\n",
    "        if keyword in tweet[0]:\n",
    "            foreign_politics_topic.append(tweet)\n",
    "\n",
    "            # json_file['topic'] = 'foreign_politics'\n",
    "            # foreign_politics_json.append(json_file.copy())\n",
    "\n",
    "            control += 1\n",
    "            break\n",
    "\n",
    "    for keyword in internal_politics_keywords:\n",
    "        if keyword in tweet[0]:\n",
    "            internal_politics_topic.append(tweet)\n",
    "\n",
    "            # json_file['topic'] = 'internal_politics'\n",
    "            # internal_politics_json.append(json_file.copy())\n",
    "\n",
    "            control += 1\n",
    "            break\n",
    "\n",
    "    for keyword in trump_keywords:\n",
    "        if keyword in tweet[0]:\n",
    "            trump_4pre_topic.append(tweet)\n",
    "\n",
    "            # json_file['topic'] = 'trump_politcs'\n",
    "            # trump_4pre_json.append(json_file.copy())\n",
    "\n",
    "            control += 1\n",
    "            break\n",
    "\n",
    "    if control == 0:\n",
    "        not_got.append(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_got[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(not_got)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* As we can see from the 5 documents on the not_got, they don't really refer to politics. Luckily, just a little more of 150 documents where outsiders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment of the tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We wanted to study the sentiments that these tweets contain. We started with an easy inspection.\n",
    "* We downloaded the [NRC Emotion Lexicon](http://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm) kindly offered by the National Research Council Canada, created by Saif Mohammad. We used the 'NRC-Emotion-Lexicon-Wordlevel-v0.92' dataset, that contains 14.000 words. Every word is associated with 8 emotions and a positive or negative value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment of the tweets associated to a topic\n",
    "\n",
    "* We searched for every word in every tweet and we summed the values of every sentiment associated.\n",
    "* It takes a few time to compute every sentiment for every tweet: you can decide to run the following cell or to run the next one that load the pre-computed dataframe from a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This is an helper function that load the 'lexicons' (word -> emotions) from a file and returns\n",
    "a pandas DataFrame with a unique column 'term' and other 10 columns that are the emotions associated.\n",
    "'''\n",
    "# lexicons = load_lexicons()\n",
    "\n",
    "'''Loading files'''\n",
    "# dir_path = 'topic_tweets'\n",
    "# directory = os.fsencode(dir_path)\n",
    "# \n",
    "# first = True\n",
    "# tweets = pd.DataFrame()\n",
    "# for file in os.listdir(directory):\n",
    "#     filename = os.fsdecode(file)\n",
    "#     if filename == '.DS_Store':\n",
    "#         continue\n",
    "#     if first:\n",
    "#         tweets = pd.read_json(dir_path + '/' + filename)\n",
    "#         first = False\n",
    "#     else:\n",
    "#         tweets = tweets.append(pd.read_json(dir_path + '/' + filename))\n",
    "# \n",
    "# length = lexicons.shape[0]\n",
    "\n",
    "# We create a column for every emotion\n",
    "# tweets['anger'] = 0\n",
    "# tweets['anticipation'] = 0\n",
    "# tweets['disgust'] = 0\n",
    "# tweets['fear'] = 0\n",
    "# tweets['joy'] = 0\n",
    "# tweets['negative'] = 0\n",
    "# tweets['positive'] = 0\n",
    "# tweets['sadness'] = 0\n",
    "# tweets['surprise'] = 0\n",
    "# tweets['trust'] = 0\n",
    "# \n",
    "# for index, lexicon in lexicons.iterrows():\n",
    "#     # print('Iterations to the end: %s...' % (length - index))\n",
    "# \n",
    "#     tweets_with_lexicon = tweets['text'].str.contains(lexicon.term, case=False)\n",
    "# \n",
    "#     # We take the emotions\n",
    "#     emotions = lexicon.drop(['term'])\n",
    "# \n",
    "#     # For every emotion we update the value in the df\n",
    "#     if any(tweets_with_lexicon.values):\n",
    "#         for attribute, value in zip(emotions.index.values, emotions.values):\n",
    "#             if value != 0:\n",
    "#                 tweets.loc[tweets_with_lexicon, attribute] += value\n",
    "\n",
    "# tweets.to_csv('data/tweets_topic_em.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets_topic_em = pd.read_csv('data/tweets_topic_em.csv', index_col=0)\n",
    "tweets_topic_em.set_index('id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_topic_em.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_topic_em.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* What we are going to do now is to group the dataframe by the topic and see the differences between the sentiments in the different topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentiment = tweets_topic_em.drop(['created_at', 'text']).groupby('topic').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3, 3, figsize=(20,22))\n",
    "\n",
    "sentiment.loc['business'].drop(['favorite_count', 'retweet_count']).plot(kind='bar', ax=axs[0,0])\n",
    "axs[0,0].set_title(\"BUSINESS\")\n",
    "\n",
    "sentiment.loc['china'].drop(['favorite_count', 'retweet_count']).plot(kind='bar', ax=axs[0,1])\n",
    "axs[0,1].set_title(\"CHINA\")\n",
    "\n",
    "sentiment.loc['foreign_politics'].drop(['favorite_count', 'retweet_count']).plot(kind='bar', ax=axs[0,2])\n",
    "axs[0,2].set_title(\"FOREIGN POLITICS\")\n",
    "\n",
    "sentiment.loc['hillary'].drop(['favorite_count', 'retweet_count']).plot(kind='bar', ax=axs[1,0])\n",
    "axs[1,0].set_title(\"HILLARY CLINTON\")\n",
    "\n",
    "sentiment.loc['internal_politics'].drop(['favorite_count', 'retweet_count']).plot(kind='bar', ax=axs[1,1])\n",
    "axs[1,1].set_title(\"INTERNAL POLITICS\")\n",
    "\n",
    "sentiment.loc['interviews_debates'].drop(['favorite_count', 'retweet_count']).plot(kind='bar', ax=axs[1,2])\n",
    "axs[1,2].set_title(\"INTERVIEWS\")\n",
    "\n",
    "sentiment.loc['obama'].drop(['favorite_count', 'retweet_count']).plot(kind='bar', ax=axs[2,0])\n",
    "axs[2,0].set_title(\"OBAMA\")\n",
    "\n",
    "sentiment.loc['shows'].drop(['favorite_count', 'retweet_count']).plot(kind='bar', ax=axs[2,1])\n",
    "axs[2,1].set_title(\"SHOWS\")\n",
    "\n",
    "sentiment.loc['various'].drop(['favorite_count', 'retweet_count']).plot(kind='bar', ax=axs[2,2])\n",
    "axs[2,2].set_title(\"VARIOUS\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can see that generally Trump have more positive tweets than negative, he needs people to like him for his business and for his politics. More specifically we can find a confirm of this fact lookin at the sentiments in Business, Shows and Interviews. Of course, when he speaks about his business, he has to be positive: we can see an high level of trust in his tweets. For what concerns shows, we can say very similar things: he produces and leads a tv show called 'Celebrity Apprentice' and he often tweets about it. He needs to transmit positive vibes to convince people that his show is worth watching. A very similar pattern for similar reason is found in the interviews and debate topic.\n",
    "* For what concerns politics we have different trends: there are high level of anger and fear, with an high level of trust, in every topic concerning politics except for Obama, that is a little different. When it comes to politics he generally have slightly more negative tweets than positive. The interesting thing is that his level of trust is always  highest in every topic compared to the other emotions as we expected, except when he talks about Hillary Clinton or Barack Obama.\n",
    "* Clinton: the highest emotion are anger, fear, disgust and sadness in this order. Not that it's not something that we were not expecting, but the fact that the trust level is lower than four other negative emotions it's very relevant.\n",
    "* Obama: here positive and negative emotions are almost at the same level, even if sadness is the highest emotion we can find. Levels of trust are second, so differently from Clinton topic, his more trustful when he speaks about Obama than Clinton.\n",
    "* Note that the levels of joy are often low compared to the other emotions, except for shows and business."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now that we have seen Trump's emotions in his tweets, it would be curious to discover what the Twitter's users think about those tweets. In order to understand that, we are going to analyze the number of favorite and retweet counts, that is the only information we have about Trump's public."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(20,12))\n",
    "x = np.array([0,1,2,3,4,5,6,7,8])\n",
    "ax.bar(x - 0.2, sentiment.loc[:,'favorite_count'], width=0.2, color ='b')\n",
    "ax.bar(x + 0.2, sentiment.loc[:,'retweet_count'], width=0.2, color ='r')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(sentiment.index.values)\n",
    "ax.set_xlabel('topic')\n",
    "ax.set_ylabel('number of retweets/favorites')\n",
    "ax.set_title(\"Number of retweets and favorites in different topics\")\n",
    "ax.legend(['favorite count', 'retweet count'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can clearly see that the number of retweet and favorite are way higher when they are about Hillary Clinton. These tweets are from 2016 and 2017, so as we know, during the election campaign Trump's tweets reached a very high popularity.\n",
    "\n",
    "* We would like to see when the other topics with \"high\" count (internal and foreign politics and interviews and debates) of favorite and retweet count reached their maximum peaks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize=(15,8), sharey=True)\n",
    "\n",
    "int_pol = tweets_topic_em[tweets_topic_em.topic == 'internal_politics']\n",
    "int_pol.created_at = int_pol.created_at.apply(pd.to_datetime)\n",
    "int_pol.reset_index(inplace=True)\n",
    "\n",
    "for_pol = tweets_topic_em[tweets_topic_em.topic == 'foreign_politics']\n",
    "for_pol.created_at = for_pol.created_at.apply(pd.to_datetime)\n",
    "for_pol.reset_index(inplace=True)\n",
    "\n",
    "interv = tweets_topic_em[tweets_topic_em.topic == 'interviews_debates']\n",
    "interv.created_at = interv.created_at.apply(pd.to_datetime)\n",
    "interv.reset_index(inplace=True)\n",
    "\n",
    "int_pol.plot(kind='area', x='created_at', y=['favorite_count','retweet_count'],\n",
    "             ax=axs[0], title='User reaction for internal politics in time')\n",
    "for_pol.plot(kind='area', x='created_at', y=['favorite_count','retweet_count'],\n",
    "             ax=axs[1], title='User reaction for foreign politics in time')\n",
    "interv.plot(kind='area', x='created_at', y=['favorite_count','retweet_count'],\n",
    "             ax=axs[2], title='User reaction for interviews and debates in time')\n",
    "plt.gcf().autofmt_xdate()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* It's clear that during the campaign and his first periods of presidency, Trump's tweets popularity arised with a very strong trend."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment of every tweet (not from iPhone)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Following the same method that we used before, we created a json file that contains every tweet with the associated emotions. Now we are interested in a more general and larger view of the emotions of the tweets of Trump."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'data/both_tweets_and_emotion_on_all.json'\n",
    "\n",
    "with open(data_path) as f:\n",
    "    data = json.load(f)\n",
    "all_tweets = pd.DataFrame(data)\n",
    "all_tweets.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mean_all = all_tweets.describe().drop(['id', 'in_reply_to_user_id_str'], axis=1).loc['mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,7))\n",
    "mean_all.drop(['favorite_count', 'retweet_count']).plot(kind='bar', title='Average of the emotions on all tweets', ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can see that our assumption that Trump's tweets are generally more positive holds. As we already stated studying the topics, the trust emotion is generally the most relevant and joy is the lowest with surprise. The other emotions lay on the same level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Usage\n",
    "#### Add viz next milestone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now that we have different topics, we are curious to see what are the most used words in every topic (better visualization are coming soon!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loading the stopwords that remove useless words, like 'the', 'do', 'a', 'an', 'one'...\n",
    "with open('stopwords/stopwords_wordcount.json', 'r') as fp:\n",
    "    stopwords_wordcount = json.load(fp)\n",
    "\n",
    "# We are going to take only words and not punctuation\n",
    "strip_special_chars = re.compile(\"[^A-Za-z0-9 ]+\")\n",
    "def clean_sentences(string):\n",
    "    string = string.lower().replace(\"<br />\", \" \")\n",
    "    return re.sub(strip_special_chars, \"\", string.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topic_word_count = {}\n",
    "topics = tweets_topic_em.topic.unique()\n",
    "for topic in topics:\n",
    "    # We create a dictionary for every topic\n",
    "    topic_dict = {}\n",
    "    # Df with all the tweets from that topic\n",
    "    temp = tweets_topic_em[tweets_topic_em.topic == topic]\n",
    "    \n",
    "    # We clean the tweet, we split it in words and we remove the stopwords\n",
    "    for index, row in temp.iterrows():\n",
    "        text = clean_sentences(row.text.lower()).split()\n",
    "\n",
    "        text = [word for word in text if word not in stopwords]\n",
    "        \n",
    "        # Creating tuples (word, count)\n",
    "        counter = collections.Counter(text)\n",
    "        for word, count in counter.items():\n",
    "\n",
    "            if word in topic_dict:\n",
    "                topic_dict[word] += count\n",
    "            else:\n",
    "                topic_dict[word] = count\n",
    "                \n",
    "    # Saving the dictionary sorted by highest count to lowest\n",
    "    ordered_dict = dict(sorted(topic_dict.items(), key=operator.itemgetter(1), reverse=True))\n",
    "    topic_word_count[topic] = ordered_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (key, value) in topic_word_count.items():\n",
    "    print('TOPIC: ', key.upper())\n",
    "    print('The 20 most used words are: ')\n",
    "    i = 20\n",
    "    for (word, count) in value.items():\n",
    "        print(word + ': ' + str(count))\n",
    "        i -= 1\n",
    "        if i == 0:\n",
    "            break\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='phone_analysis'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of source usage (android iPhone)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part we decided to analyze the main source from which the tweets were sent during campaign period and the presidency period. We decided to do it in order to see if there is a major change in the source usage from the results obtained in the article already cited: http://varianceexplained.org/r/trump-tweets/. This is because the article was published on the 9th of August 2016.\n",
    "- First, we will create separate columns for tweets from iphone and tweets from android\n",
    "- Then, we will group by month and we apply the count method to the new columns\n",
    "- Finally, we will show a bar plot for the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we retrieve the data for the two periods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieving all data from Trump's tweets dataset\n",
    "all_data, condensed, master = load_data()\n",
    "\n",
    "# getting the condensed version for year 2016 and 2017\n",
    "condensed_2016 = all_data[\"condensed_2016\"]\n",
    "condensed_2017 = all_data[\"condensed_2017\"]\n",
    "\n",
    "# creating a dataframe for campaign period\n",
    "cond_US_campaign_2016 = select_time_interval(condensed_2016, 'created_at',\n",
    "                                             np.datetime64('2016-02-01'), np.datetime64('2016-11-08'))\n",
    "\n",
    "cond_US_campaign_2016 = cond_US_campaign_2016.sort_values('created_at')\n",
    "\n",
    "\n",
    "# creating a dataframe for presidency period\n",
    "cond_president_period_df = select_time_interval(condensed_2017, 'created_at',\n",
    "                                                np.datetime64('2017-01-20'), np.datetime64('2017-11-05'))\n",
    "\n",
    "cond_president_period_df = cond_president_period_df.sort_values('created_at')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here we add a column to both dataframe with the month for each tweet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cond_US_campaign_2016['Month'] = cond_US_campaign_2016['created_at'].dt.month\n",
    "\n",
    "cond_president_period_df['month'] = cond_president_period_df['created_at'].dt.month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we start with the campaign period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweets from Iphone source\n",
    "tweets_iphone_campaign_df = cond_US_campaign_2016.loc[cond_US_campaign_2016['source']\n",
    "                                                      == 'Twitter for iPhone', ['Month', 'source']]\n",
    "\n",
    "tweets_iphone_campaign_df.rename(columns={'source': 'tweets_from_iphone'}, inplace=True)\n",
    "\n",
    "# tweets from android source\n",
    "tweets_android_campaign_df = cond_US_campaign_2016.loc[cond_US_campaign_2016['source']\n",
    "                                                       == 'Twitter for Android', ['Month', 'source']]\n",
    "\n",
    "tweets_android_campaign_df.rename(columns={'source': 'tweets_from_android'}, inplace=True)\n",
    "\n",
    "# final dataframe with the two new columns\n",
    "tweets_source_campaign_df = tweets_iphone_campaign_df.append(tweets_android_campaign_df)\n",
    "\n",
    "# here we group by month and use the count method on the two columns\n",
    "tweets_source_campaign_month = tweets_source_campaign_df.groupby('Month').agg({'tweets_from_iphone': 'count',\n",
    "                                                                               'tweets_from_android': 'count'})\n",
    "# reorder the dataframe by month\n",
    "tweets_source_campaign_month.sort_index(inplace=True)\n",
    "\n",
    "# creating a bar chart to visualize our results\n",
    "barchart_tweets_source = tweets_source_campaign_month.plot(kind='bar', title=\"tweets source campaign\",\n",
    "                                                           legend=True, fontsize=12, figsize=(20, 10))\n",
    "\n",
    "barchart_tweets_source.set_xlabel(\"Month\", fontsize=12)\n",
    "barchart_tweets_source.set_ylabel(\"number of tweets\", fontsize=12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nothing in particular can be noticed, as this period was almost completely covered by the article."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now, we will do the same for the presidency period:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweets from Iphone source\n",
    "tweets_iphone_presidency_df = cond_president_period_df.loc[cond_president_period_df['source']\n",
    "                                                           == 'Twitter for iPhone', ['month', 'source']]\n",
    "\n",
    "tweets_iphone_presidency_df.rename(columns={'source': 'tweets_from_iphone'}, inplace=True)\n",
    "\n",
    "# tweets from android source\n",
    "tweets_android_presidency_df = cond_president_period_df.loc[cond_president_period_df['source']\n",
    "                                                           == 'Twitter for Android', ['month', 'source']]\n",
    "\n",
    "tweets_android_presidency_df.rename(columns={'source': 'tweets_from_android'}, inplace=True)\n",
    "\n",
    "# final dataframe with the two new columns\n",
    "tweets_source_presidency_df = tweets_iphone_presidency_df.append(tweets_android_presidency_df)\n",
    "\n",
    "\n",
    "# now we group by month and we count the number of tweets for the two columns\n",
    "tweets_source_presidency_month = tweets_source_presidency_df.groupby('month').agg({'tweets_from_iphone': 'count',\n",
    "                                                                                   'tweets_from_android': 'count'})\n",
    "# we sort the dataframe by month\n",
    "tweets_source_presidency_month.sort_index(inplace=True)\n",
    "\n",
    "# we create a bar chart to visualize our results\n",
    "barchart_tweets_source_pres = tweets_source_presidency_month.plot(kind='bar', title=\"tweets source presidency\",\n",
    "                                                                legend=True, fontsize=12, figsize=(20, 10))\n",
    "\n",
    "barchart_tweets_source_pres.set_xlabel(\"Month\", fontsize=12)\n",
    "barchart_tweets_source_pres.set_ylabel(\"number of tweets\", fontsize=12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surprisingly, it is clear that in March 2017, the president switched completely to iPhone. Obviously this fact cannot be reached by the analysis of the article, it was published the 9th of August 2016. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Fake news term usage analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this task we decided to consider three different period of time in order to see when the term \"fake news\" arise. We took the campaign period, the president-elect period and the presidency period. The main goal of this part is to see when the president started using this term, how many times he uses it and see if we can get a correlation between when he uses the term and some specific situations as scandals or periods of high pressure on his work and his administration. First of all we create a dataframe for each of these periods. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here we create the three dataframes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# retrieving all data from Trump's tweets dataset\n",
    "all_data, condensed, master = load_data()\n",
    "\n",
    "# getting the condensed version for year 2016 and 2017\n",
    "condensed_2016 = all_data[\"condensed_2016\"]\n",
    "condensed_2017 = all_data[\"condensed_2017\"]\n",
    "\n",
    "# creating a dataframe for campaign period\n",
    "cond_US_campaign_2016 = select_time_interval(condensed_2016, 'created_at',\n",
    "                                             np.datetime64('2016-02-01'), np.datetime64('2016-11-08'))\n",
    "\n",
    "cond_US_campaign_2016 = cond_US_campaign_2016.sort_values('created_at')\n",
    "\n",
    "\n",
    "# creating a dataframe for president elect period\n",
    "cond_pres_elect_df = select_time_interval(condensed_2016, 'created_at',\n",
    "                                          np.datetime64('2016-11-09'), np.datetime64('2016-12-31'))\n",
    "\n",
    "cond_pres_elect_df_2017 = select_time_interval(condensed_2017, 'created_at',\n",
    "                                               np.datetime64('2017-01-01'), np.datetime64('2017-01-20'))\n",
    "\n",
    "cond_pres_elect_df = cond_pres_elect_df.append(cond_pres_elect_df_2017)\n",
    "\n",
    "cond_pres_elect_df = cond_pres_elect_df.sort_values('created_at')\n",
    "\n",
    "\n",
    "# creating a dataframe for presidency period\n",
    "cond_president_period_df = select_time_interval(condensed_2017, 'created_at',\n",
    "                                                np.datetime64('2017-01-20'), np.datetime64('2017-11-05'))\n",
    "\n",
    "cond_president_period_df = cond_president_period_df.sort_values('created_at')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The campaign period"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now we will search in each tweets for the term \"fake news\". We used a simple regex and we do not consider case sensitive. We create a new column of boolean values 'fake_news_used' in our dataframe using the 'contains' method. After this we create useful columns with Month, week/year and day values in order to make a groupby with them and see some interesting patterns in the usage of the term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# creating the column with boolean values for the matches of the regex:\n",
    "cond_US_campaign_2016['fake_news_used'] = cond_US_campaign_2016['text'].str.contains('fake news|fakenews', case=False)\n",
    "\n",
    "# creating the columns 'Month', 'week/year' and 'Date'\n",
    "cond_US_campaign_2016['Month'] = cond_US_campaign_2016['created_at'].dt.month\n",
    "cond_US_campaign_2016['week/year'] = cond_US_campaign_2016['created_at'].apply(lambda x: \"%d/%d\" % (x.week, x.year))\n",
    "cond_US_campaign_2016['Date'] = cond_US_campaign_2016['created_at'].dt.date\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now first we count how many positive results we had:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond_US_campaign_2016['fake_news_used'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, during his campaign period, the 'fake news' did not appear in any of his tweets. In order to be sure we search for the words 'fake' and 'news' separately and these are the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search for the word fake\n",
    "match_df = cond_US_campaign_2016.loc[:,['text']]\n",
    "match_df['fake_usage'] = cond_US_campaign_2016['text'].str.contains('fake', case=False)\n",
    "temp_df = match_df[match_df['fake_usage'] == True]\n",
    "\n",
    "print(str(temp_df.text.values))\n",
    "temp_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search for the word 'news'\n",
    "match_df = cond_US_campaign_2016.loc[:,['text']]\n",
    "match_df['news_usage'] = cond_US_campaign_2016['text'].str.contains('news', case=False)\n",
    "temp_df = match_df[match_df['news_usage'] == True]\n",
    "\n",
    "# showing the results\n",
    "temp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of positive matches:\n",
    "temp_df.news_usage.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the 'news' term we found mostly tweets with hashtags of media or other tweets that are related with 'fake news' term.\n",
    "Therefore, we conlcude what we have said before, in the campaign period there is no sign of the 'fake news' term in his tweets. We can go ahead with the president-elect period."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### President elect period"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here we repeat the same process that we have done for the campaign period:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the column with boolean values for the matches of the regex:\n",
    "cond_pres_elect_df['fake_news_used'] = cond_pres_elect_df['text'].str.contains('fake news|fakenews', case=False)\n",
    "\n",
    "# creating the columns 'month', 'week/year' and 'date'\n",
    "cond_pres_elect_df['month'] = cond_pres_elect_df['created_at'].dt.month\n",
    "cond_pres_elect_df['week/year'] = cond_pres_elect_df['created_at'].apply(lambda x: \"%d/%d\" % (x.week, x.year))\n",
    "cond_pres_elect_df['date'] = cond_pres_elect_df['created_at'].dt.date\n",
    "\n",
    "# showing the number of positive matches:\n",
    "cond_pres_elect_df['fake_news_used'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We have few positive matches, let's continue with the analysis:\n",
    "- first we display all the tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_df = cond_pres_elect_df.loc[cond_pres_elect_df['fake_news_used'] == True, ['date','text']]\n",
    "match_df = match_df.sort_values('date')\n",
    "from IPython.display import display\n",
    "print(str(match_df.text.values))\n",
    "match_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We noticed that in this 11 tweets the fake news term was related mostly with CNN and Russia. Specially the rise of the term was due to release of a non-verified paper by Buzzfeed containing strong claims about Trump and Russia ties and possibility that Trump could be blackmailed by the Russian governement. He over reacted to this leak, attacking the intelligence agencies and trying to discredit the Media. For more information, here you have the story described by the New York Times: https://www.nytimes.com/2017/01/10/business/buzzfeed-donald-trump-russia.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Due to the small number of tweets with the 'fake news' term we can go ahead with the next period."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Presidency period"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we prepare the dataframe creating the column for the match:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the column with boolean values from the regex result:\n",
    "cond_president_period_df['fake_news_used'] = cond_president_period_df['text'].str.contains('fake news|fakenews', case=False)\n",
    "\n",
    "# showing the number of positive results:\n",
    "cond_president_period_df['fake_news_used'].sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Considered that we have a period of 10 month, we have an interesting number of postive matches, so we can start a deeper analysis conidering the months, the week/year and by date:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# adding month, date and week/year columns in order to make groupby operations\n",
    "cond_president_period_df['month'] = cond_president_period_df['created_at'].dt.month\n",
    "cond_president_period_df['date'] = cond_president_period_df['created_at'].dt.date\n",
    "cond_president_period_df['week/year'] = cond_president_period_df['created_at'].apply(lambda x: \"%d/%d\" % (x.week, x.year))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis by month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For a first insight, we will groupby by Month columns and count the usage of the term per month, showing it with a bar plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_analysis_df = cond_president_period_df[['month', 'fake_news_used']]\n",
    "month_analysis_df = month_analysis_df.groupby(['month']).sum()\n",
    "\n",
    "bar_month_plot = month_analysis_df.plot(kind='bar', title=\"fake news term usage by month\", legend=True, fontsize=12, figsize=(14, 7))\n",
    "bar_month_plot.set_xlabel(\"Month\", fontsize=12)\n",
    "bar_month_plot.set_ylabel(\"count\", fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We reorder the dataset by the number of times he used the term and show again the plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_analysis_df = month_analysis_df.sort_values('fake_news_used', ascending=False)\n",
    "\n",
    "#showing the dataframe\n",
    "month_analysis_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# showing the bar plot\n",
    "bar_month_plot = month_analysis_df.plot(kind='bar', title=\"fake news term usage by month\", legend=True, fontsize=12, figsize=(14, 7))\n",
    "bar_month_plot.set_xlabel(\"Month\", fontsize=12)\n",
    "bar_month_plot.set_ylabel(\"count\", fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It is to be said that we have small values for january 2017 because the presidency period starts the 20th of january.\n",
    "- dividing by month it is clear that we need to restrict the period of time of the groupby in order to see if we can obtain clear peak in the term usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Before moving on the week/year period we want to see the tweets in the month of october in order to see if they are all tied with a particular fact:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the matched tweets in October:\n",
    "month_match_df = cond_president_period_df.loc[cond_president_period_df['fake_news_used'] == True, ['month', 'text']]\n",
    "\n",
    "# showing all the tweets:\n",
    "print(str(month_match_df[month_match_df['month'] == 10].text.values))\n",
    "\n",
    "month_match_df.loc[month_match_df['month'] == 10, ['text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we imagined, the tweets are not related to only one main topic or event, but it is interesting to note that some of them they are related to these events: \n",
    "- the problematic situation with the claims made by the Secretary of State Rex Tillerson, namely that he thought about quitting is role and that he called the president \"a moron\", as reported in an article of NBC news: https://www.nbcnews.com/politics/white-house/tillerson-s-fury-trump-required-intervention-pence-n806451\n",
    "- Reports about the unhappiness and the risk of resign of the White House chief of staff John Kelly: http://edition.cnn.com/2017/10/12/politics/john-kelly-unusual-news-conference/index.html\n",
    "- new developments in the investigation of Russia influence in the USA election 2016, where A former Trump campaign foreign policy adviser confessed about making a false statement to the FBI after he lied about his interactions with foreign officials close to the Russian government: https://www.theguardian.com/us-news/2017/oct/30/george-papadopoulos-trump-campaign-aide-pleaded-guilty-lying-agents-russia-inquiry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This could be a first clue about the relationship between the term usage in his tweets and specific problematic situations for him (scandals or pressure for his administration). Now we will procede using the week interval, hoping to find more clear insights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### analysis by week/year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's refine our analysis considering the week/year:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "week_analysis_df = cond_president_period_df[['week/year', 'fake_news_used']]\n",
    "week_analysis_df = week_analysis_df.groupby(['week/year']).sum()\n",
    "\n",
    "bar_week_plot = week_analysis_df.plot(kind='bar', title=\"fake news term usage by week\", legend=True, fontsize=12, figsize=(14, 7))\n",
    "bar_week_plot.set_xlabel(\"week\", fontsize=12)\n",
    "bar_week_plot.set_ylabel(\"count\", fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We have two clear peaks: the 26th week of the year (from June 26, 2017 to July 2, 2017) and the 39th week of the year (from September 25, 2017 to October 1, 2017).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- let us see the tweets of those two weeks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing the matched tweets in the 26th week\n",
    "week_analysis_df = cond_president_period_df.loc[cond_president_period_df['fake_news_used'] == True, ['week/year', 'text']]\n",
    "week_26 = week_analysis_df.loc[week_analysis_df['week/year'] == '26/2017', ['text']]\n",
    "\n",
    "print(str(week_26.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Trump again over reacted about an article by CNN that was proposed as an investigation into a meeting between an associate of Donald Trump (Anthony Scaramucci) and the head of a Russian investment fund. The article was removed from the website after a day of the publication because it was written without following any check or standard. As result, three CNN employees resigned and Trump used this situation to strongly attack the media again. For more information: https://www.theguardian.com/media/2017/jun/27/three-cnn-journalists-resign-over-retracted-trump-russia-story"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- now we consider the 39th week:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing the matched tweets in the 26th week\n",
    "week_39 = week_analysis_df.loc[week_analysis_df['week/year'] == '39/2017', ['text']]\n",
    "\n",
    "print(str(week_39.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the tweets are mostly dealing with the situation in Puerto Rico following the devastation caused by Hurricane Maria. Trump felt attacked becuase his administration was blamed of not taking adequate decisions in order to help Puerto Rico. He has come back to the attention of the media after he attacked the mayor of San Juan, Carmen Yulín Cruz (Cruz critized Trump administration about how they wer dealing the complex situation). Again, we can see this link between the usage of the term \"fake news\" after he is been under accuse or under stress situation; Trump over reacted and he tried to divert attention from the main problem by discrediting the media and by dividing the community. For more information see the following links:\n",
    "- https://www.washingtonpost.com/news/the-fix/wp/2017/09/30/trump-doesnt-get-it-on-puerto-rico-he-just-proved-it-by-lashing-out-at-san-juans-mayor/?utm_term=.693061c95fe5\n",
    "- http://edition.cnn.com/2017/10/01/politics/trump-tweets-puerto-rico/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "It will be interesting to analyse the other weeks that have a good number of times of usage, we are working on it and this is one of the two last step that we have to complete for this part before Milestone 3. Now we will move on with the analysis by date."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### analysis by date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will again create a dataframe with the number of times he used the term in day, then we will sort the dataframe by the number of times he used the term, after this we will also show the results in a bar plot:\n",
    "given the great number of dates, we decided to make a plot for each month with on x-axis the days of the month and on y-axis the number of time he used the term in that specific date.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here, we will group by date and then sort the dataframe in descending order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "day_analysis_df = cond_president_period_df[['date', 'fake_news_used']]\n",
    "\n",
    "# computing the number of times he used the word by day\n",
    "day_analysis_df = day_analysis_df.groupby(['date']).sum()\n",
    "\n",
    "# sorting the dataframe\n",
    "day_analysis_df = day_analysis_df.sort_values('fake_news_used', ascending=False)\n",
    "\n",
    "day_analysis_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first three entries will be deepen after we show the bar plot:\n",
    "- First, we group by the month and the date and we obtain the number of times.\n",
    "- Then, for each month, we show the bar plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day_analysis_df = cond_president_period_df[['month','date', 'fake_news_used']]\n",
    "\n",
    "# computing the number of times he used the word by day\n",
    "day_analysis_df = day_analysis_df.groupby(['month','date']).sum()\n",
    "\n",
    "months = day_analysis_df.index.get_level_values('month').unique()\n",
    "\n",
    "# here, we create a dataframe for each month:\n",
    "dataframe_month_list = []\n",
    "for month in months:\n",
    "    dataframe_month_list.append(day_analysis_df.iloc[day_analysis_df.index.get_level_values('month') == month])\n",
    "    \n",
    "# here, for each dataframe created we show the bar plot:\n",
    "for df in dataframe_month_list:\n",
    "    df.reset_index(level=0, drop=True, inplace= True)\n",
    "    df.plot(kind='bar', title=\"fake news term usage by day\",legend=True, fontsize=12,\n",
    "            figsize=(20, 10)).set_xlabel(\"day\", fontsize=12)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "from the sorting and the plots we decided to deepen the following three dates (2017-09-30, 2017-05-28 and 2017-08-07).\n",
    "We will show the tweets and try to elaborate on them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the specific date for the matching mask\n",
    "date = datetime.datetime.strptime('2017 05 28', '%Y %m %d').date()\n",
    "\n",
    "# getting the matched tweets in the day 2017-05-28\n",
    "day_analysis_df = cond_president_period_df.loc[cond_president_period_df['fake_news_used'] == True, ['date', 'text']]\n",
    "\n",
    "date_2017_05_28 = day_analysis_df.loc[day_analysis_df['date'] == date, ['text']]\n",
    "print(str(date_2017_05_28.text.values))\n",
    "date_2017_05_28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As you can see we have three tweets repeated (we do not know right now if it is due to some repetition in our original dataset, further search will be done). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- These tweets are linked again to another difficult situation for president Trump, namely, the scandal about his son in law Jared Kushner and his ties with the russian government. It was then reported that Jared Kushner, had asked the Russian ambassador to the US to create a secret channel with Russia using Russia’s communication systems. The communications were intercepted by US intelligence and then leaked to The Washington Post. President Trump dealt with this problem trying again to label a scandal as a fake news, again he tried to take the focus away from the main situation. For further information: http://www.independent.co.uk/news/world/americas/us-politics/donald-trump-washington-mounting-crisis-jared-kusher-backchannel-russia-latest-a7760481.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now, we will procede with the date 2017-08-07:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the specific date for the matching mask\n",
    "date = datetime.datetime.strptime('2017 08 07', '%Y %m %d').date()\n",
    "\n",
    "# getting the matched tweets in the day 2017-08-07\n",
    "day_analysis_df = cond_president_period_df.loc[cond_president_period_df['fake_news_used'] == True, ['date', 'text']]\n",
    "\n",
    "date_2017_08_07 = day_analysis_df.loc[day_analysis_df['date'] == date, ['text']]\n",
    "print(str(date_2017_08_07.text.values))\n",
    "date_2017_08_07"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- President trump attacked again the media after Deputy Attorney General Rod J. Rosenstein told that if the special consuel for the Russian sabotage of USA 2016 election finds evidence of crimes, the consuel can investigate them. This fact create pressure for president Trump, who decided again to blame the media (this time accused of trying to weaken up the support for the president) in order to reassure his position and hide the main problem. \n",
    "- For more information: https://www.washingtonpost.com/blogs/plum-line/wp/2017/08/07/as-mueller-closes-in-trump-prepares-his-base-for-the-worst/?utm_term=.29b36bdeaae9\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the tweets from the date 2017-09-30:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the specific date for the matching mask\n",
    "date = datetime.datetime.strptime('2017 09 30', '%Y %m %d').date()\n",
    "\n",
    "# getting the matched tweets in the day 2017-09-30\n",
    "day_analysis_df = cond_president_period_df.loc[cond_president_period_df['fake_news_used'] == True, ['date', 'text']]\n",
    "\n",
    "date_2017_09_30 = day_analysis_df.loc[day_analysis_df['date'] == date, ['text']]\n",
    "print(str(date_2017_09_30.text.values))\n",
    "date_2017_09_30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we could imagine by the week of this date, it came out that these tweets are the tweets referring to the Puerto Rico situation during the disaster of the hurricane Maria. See previous comments for remind."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Last tasks proposed for this part before the Milestone 3\n",
    "\n",
    "- Before Milestone 3 we want to refine and deepen these correlations between the term usage and the facts that happened in the same period, we will concetrate more on the week/year intervals.\n",
    "- Currently, we are working on doing a sentiment analysis only on the tweets that contain the 'fake news' term, in order to see if we can get a clear mood of his tweets when they are about fake news.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Washington Post dataset of debunked Trump's claims"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By dealing with the issue of fake news, it is natural to ask how many times Trump has not been accurate in expressing facts and truths. As a result, during the preparatory research of our data, we tried to figure out if there was a way to have data in which Trump's tweets had been unveiled or corrected because they were unreliable. After several tests we found a washingtonPost dataset where there are Trump statements from different sources (interviews, declarations, tweets, etc.) and related to each of them there is the debunked version by the washingtonPost. Since we've decided to analyze only the source of the tweets, we've only taken the parts about tweets. Below we will explain the scraping part for data capture and the resulting merge with our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we begin by listing the link from the page from which we collected the data (the page is very interesting, with a good and clear date visualization). Link: https://www.washingtonpost.com/graphics/politics/trump-claims-database/?utm_term=.e3895bf3f60d\n",
    "\n",
    "- First of all, as you can see on the page, the data is loaded dynamically and you need to select filters to only get data regarding the tweets (as well as loading the data list multiple times to view them all). For this reason, at the beginning we had thought of using Selenium to emulate clicks on the page, but it would be time-consuming since it's simply possible to select the filters, upload all the data and then download the obtained html. We have followed this second choice obviously. Once we got the html, we then explored the html DOM to understand the Tags we were interested in and finally we used BeautifulSoup. Specifically the data taken for each tweet is the date, the tweet text, the washington post version, and the tweet category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here you can see the functions that we implemented for the scraping and the process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from dateutil.parser import parse as parse_date\n",
    "import json\n",
    "from itertools import chain\n",
    "\n",
    "\n",
    "def parse_claim(c):\n",
    "    '''\n",
    "    for each claim we call this method that retrieves all the information \n",
    "    that we want, namely, the date, the tweet text, the washington Post\n",
    "    analysis and the topic of the tweet.\n",
    "    '''\n",
    "    def get_topic():\n",
    "        t = c.find('div', class_='details not-expanded')\n",
    "        if not t:\n",
    "            t = c.find('div', class_='details expanded')\n",
    "        return t.find('p').text.split(': ')[-1]\n",
    "\n",
    "    date = parse_date(c.find('div', class_='dateline').text)\n",
    "    date = (date.day, date.month)\n",
    "\n",
    "    return {\n",
    "        'date': date,\n",
    "        'text': c.find('div', class_='claim').text,\n",
    "        'analysis': c.find('div', class_='analysis').text,\n",
    "        'topic': get_topic()\n",
    "    }\n",
    "\n",
    "# creates a dictionary with key the value: element[field], for element in the list_ argument\n",
    "def to_hash_collisions(list_, field):\n",
    "    res = {}\n",
    "    for element in list_:\n",
    "        key = element[field]\n",
    "        if key in res:\n",
    "            res[key].append(element)\n",
    "        else:\n",
    "            res[key] = [element]\n",
    "    return res\n",
    "\n",
    "\n",
    "def most_similar(list_, orig):\n",
    "    list_ = map(lambda text: set(text.split(' ')), list_)\n",
    "    orig = set(orig.split(' '))\n",
    "    closest_idx = -1\n",
    "    closest_dst = -1\n",
    "    for idx, bag in enumerate(list_):\n",
    "        score = len(list(filter(lambda word: word in orig, bag)))\n",
    "        if score > closest_dst:\n",
    "            closest_dst = score\n",
    "            closest_idx = idx\n",
    "    return closest_idx\n",
    "\n",
    "\n",
    "\n",
    "# Here we use BeautifulSoup on the saved html\n",
    "soup = BeautifulSoup(open('data/fake_tweets.html', encoding=\"utf8\"), 'html.parser')\n",
    "\n",
    "# we create a list of all the tweets analyzed in the page, again, we know the tags and attributes becuase we inspect the DOM \n",
    "claims_list = soup.find(id='claims-list')\\\n",
    "                  .findAll('div', class_='claim-row')\n",
    "\n",
    "# here we map the claim_list using on each element the 'parse_claim' method that we defined before\n",
    "claims = map(parse_claim, claims_list)\n",
    "\n",
    "# then we build a hash map indexed on (day,month) with collision lists of the data that we retrieved\n",
    "claims = to_hash_collisions(claims, 'date')\n",
    "\n",
    "# Read and prepare tweets from 2017\n",
    "with open('trump_tweets/condensed_2017.json', 'r') as f:\n",
    "    tweets17 = json.load(f)\n",
    "\n",
    "# we modify the column 'created_at' inserting only the day and the month.\n",
    "for t in tweets17:\n",
    "    date = parse_date(t['created_at'])\n",
    "    date = (date.day, date.month)\n",
    "    t['created_at'] = date\n",
    "\n",
    "# we Build a hash map indexed on (day,month) with collision lists for our dataset of tweets from 2017\n",
    "tweets17 = to_hash_collisions(tweets17, 'created_at')\n",
    "\n",
    "\n",
    "# Here we associate tweet id to each debunked tweet\n",
    "for date, debunked_list in claims.items():\n",
    "    if date not in tweets17:\n",
    "        continue\n",
    "        \n",
    "    collision_list = tweets17[date]\n",
    "    for deb in debunked_list:\n",
    "        corresponding = None\n",
    "        \n",
    "        # if there is only one tweet in our Trump's tweet dataset for\n",
    "        # that date we found the corresponding element\n",
    "        \n",
    "        if len(collision_list) == 1:\n",
    "            corresponding = collision_list[0]\n",
    "            \n",
    "        # else, we call our method 'most_similar' with arguments the list of tweets of that date\n",
    "        # and the tweet of that specific date from the debunked list.\n",
    "        \n",
    "        else:\n",
    "            idx = most_similar([o['text'] for o in collision_list], deb['text'])\n",
    "            corresponding = collision_list[idx]\n",
    "            \n",
    "        # finally we add the the tweet id of the matched tweet to our debunked element.\n",
    "        deb['tweet_id'] = corresponding['id_str']\n",
    "\n",
    "# Convert hashmap with collision lists back to list and sort by date\n",
    "claims = sorted(chain.from_iterable(claims.values()), key=lambda o: o['date'])\n",
    "claims = list(filter(lambda t: 'tweet_id' in t, claims))\n",
    "\n",
    "# Dump everything to a nice json\n",
    "with open('data/fact_checked.json', 'w') as f:\n",
    "    json.dump(claims, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this operation, we can add these data to our dataset of trump's tweet in 2017. We will merge the data by the tweet_id and than we will save the final dataframe in a .csv file in order to reload it for future use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge with our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading fake news debunk files that we created from WASHINGTON_POST dataset\n",
    "facts_checked = pd.read_json('data/fact_checked.json')\n",
    "\n",
    "df_facts_checked = pd.DataFrame(facts_checked)\n",
    "\n",
    "# merging our presidency dataframe with the facts_checked dataframe by the columns 'id_str' and 'tweet_id'\n",
    "cond_president_period_df = cond_president_period_df.merge(df_facts_checked, left_on='id_str', right_on='tweet_id',\n",
    "                                                          how='outer')\n",
    "\n",
    "# saving our presidency period with fact checking in a csv file\n",
    "cond_president_period_df.to_csv('data/presidency_period_with_fact_check.csv')\n",
    "\n",
    "# temp cleaned contains only the tweets with analysis, we create this only to see if the number of \n",
    "# tweets is the same from the fact_checked dataframe.\n",
    "temp_cleaned = cond_president_period_df.drop('in_reply_to_user_id_str', axis=1)\n",
    "temp_cleaned = temp_cleaned.dropna()\n",
    "\n",
    "print(\"debunked tweets shape: \" + str(df_facts_checked.shape))\n",
    "print(\"merged tweets shape: \" + str(temp_cleaned.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Last step on this part before Milestone 3 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last thing we are working on this part is to produce a word usage compared to tweeted labelled as debunked and show results, trying to see if there are differences or specific terms used in these tweets compared to all his other tweets of 2017. We decided to do so in the hope of finding results that demonstrate to the reader what terms Trump uses to create a distorted version of the facts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
